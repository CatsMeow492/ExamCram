{
  "Questions": [
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #51"
          },
          "question": {
            "S": "A manufacturing company has a large set of labeled historical sales data. The manufacturer would like to predict how many units of a particular part should be produced each quarter. Which machine learning approach should be used to solve this problem?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Logistic regression"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Random Cut Forest (RCF)"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Principal component analysis (PCA)"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Linear regression"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #52"
          },
          "question": {
            "S": "A financial services company is building a robust serverless data lake on Amazon S3. The data lake should be flexible and meet the following requirements: Support querying old and new data on Amazon S3 through Amazon Athena and Amazon Redshift Spectrum. Support event-driven ETL pipelines. Provide a quick and easy way to understand metadata. Which approach meets these requirements?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Glue ETL job, and an AWS Glue Data Catalog to search and discover metadata."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Batch job, and an external Apache Hive metastore to search and discover metadata."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Batch job, and an AWS Glue Data Catalog to search and discover metadata."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Glue ETL job, and an external Apache Hive metastore to search and discover metadata."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #53"
          },
          "question": {
            "S": "A company's Machine Learning Specialist needs to improve the training speed of a time-series forecasting model using TensorFlow. The training is currently implemented on a single-GPU machine and takes approximately 23 hours to complete. The training needs to be run daily. The model accuracy is acceptable, but the company anticipates a continuous increase in the size of the training data and a need to update the model on an hourly, rather than a daily, basis. The company also wants to minimize coding effort and infrastructure changes. What should the Machine Learning Specialist do to the training solution to allow it to scale for future demand?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Do not change the TensorFlow code. Change the machine to one with a more powerful GPU to speed up the training."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Change the TensorFlow code to implement a Horovod distributed framework supported by Amazon SageMaker. Parallelize the training to as many machines as needed to achieve the business goals."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Switch to using a built-in AWS SageMaker DeepAR model. Parallelize the training to as many machines as needed to achieve the business goals."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Move the training to Amazon EMR and distribute the workload to as many machines as needed to achieve the business goals."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #54"
          },
          "question": {
            "S": "Which of the following metrics should a Machine Learning Specialist generally use to compare/evaluate machine learning classification models against each other?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Recall"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Misclassification rate"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Mean absolute percentage error (MAPE)"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Area Under the ROC Curve (AUC)"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #55"
          },
          "question": {
            "S": "A company is running a machine learning prediction service that generates 100 TB of predictions every day. A Machine Learning Specialist must generate a visualization of the daily precision-recall curve from the predictions, and forward a read-only version to the Business team. Which solution requires the LEAST coding effort?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3. Give the Business team read-only access to S3."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3. Visualize the arrays in Amazon QuickSight, and publish them in a dashboard shared with the Business team."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Generate daily precision-recall data in Amazon ES, and publish the results in a dashboard shared with the Business team."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #56"
          },
          "question": {
            "S": "A Machine Learning Specialist is preparing data for training on Amazon SageMaker. The Specialist is using one of the SageMaker built-in algorithms for the training. The dataset is stored in .CSV format and is transformed into a numpy.array, which appears to be negatively affecting the speed of the training. What should the Specialist do to optimize the data for training on SageMaker?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Use the SageMaker batch transform feature to transform the training data into a DataFrame."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use AWS Glue to compress the data into the Apache Parquet format."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Transform the dataset into the RecordIO protobuf format."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use the SageMaker hyperparameter optimization feature to automatically optimize the data."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #57"
          },
          "question": {
            "S": "A Machine Learning Specialist is required to build a supervised image-recognition model to identify a cat. The ML Specialist performs some tests and records the following results for a neural network-based image classifier: Total number of images available = 1,000, Test set images = 100 (constant test set). The ML Specialist notices that, in over 75% of the misclassified images, the cats were held upside down by their owners. Which techniques can be used by the ML Specialist to improve this specific test error?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Increase the training data by adding variation in rotation for training images."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Increase the number of epochs for model training."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Increase the number of layers for the neural network."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Increase the dropout rate for the second-to-last layer."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #58"
          },
          "question": {
            "S": "A Machine Learning Specialist needs to be able to ingest streaming data and store it in Apache Parquet files for exploration and analysis. Which of the following services would both ingest and store this data in the correct format?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "AWS DMS"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon Kinesis Data Streams"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon Kinesis Data Firehose"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon Kinesis Data Analytics"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #59"
          },
          "question": {
            "S": "A data scientist has explored and sanitized a dataset in preparation for the modeling phase of a supervised learning task. The statistical dispersion can vary widely between features, sometimes by several orders of magnitude. Before moving on to the modeling phase, the data scientist wants to ensure that the prediction performance on the production data is as accurate as possible. Which sequence of steps should the data scientist take to meet these requirements?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Apply random sampling to the dataset. Then split the dataset into training, validation, and test sets."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Split the dataset into training, validation, and test sets. Then rescale the training set and apply the same scaling to the validation and test sets."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Rescale the dataset. Then split the dataset into training, validation, and test sets."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Split the dataset into training, validation, and test sets. Then rescale the training set, the validation set, and the test set independently."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #60"
          },
          "question": {
            "S": "A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for training, and needs to continue working for an extended period with no Wi-Fi access. Which approach should the Specialist use to continue working?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Install Python 3 and boto3 on their laptop and continue the code development using that environment."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their local environment, and use the Amazon SageMaker Python SDK to test the code."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Download TensorFlow from tensorflow.org to emulate the TensorFlow kernel in the SageMaker environment."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Download the SageMaker notebook to their local environment, then install Jupyter Notebooks on their laptop and continue the development in a local notebook."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #61"
          },
          "question": {
            "S": "A Machine Learning Specialist is working with a large cybersecurity company that manages security events in real time for companies around the world. The cybersecurity company wants to design a solution that will allow it to use machine learning to score malicious events as anomalies on the data as it is being ingested. The company also wants to be able to save the results in its data lake for later processing and analysis. What is the MOST efficient way to accomplish these tasks?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Ingest the data using Amazon Kinesis Data Firehose, and use Amazon Kinesis Data Analytics Random Cut Forest (RCF) for anomaly detection. Then use Kinesis Data Firehose to stream the results to Amazon S3."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Ingest the data into Apache Spark Streaming using Amazon EMR, and use Spark MLlib with k-means to perform anomaly detection. Then store the results in an Apache Hadoop Distributed File System (HDFS) using Amazon EMR with a replication factor of three as the data lake."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Ingest the data and store it in Amazon S3. Use AWS Batch along with the AWS Deep Learning AMIs to train a k-means model using TensorFlow on the data in Amazon S3."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Ingest the data and store it in Amazon S3. Have an AWS Glue job that is triggered on demand transform the new data. Then use the built-in Random Cut Forest (RCF) model within Amazon SageMaker to detect anomalies in the data."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #62"
          },
          "question": {
            "S": "A Data Scientist wants to gain real-time insights into a data stream of GZIP files. Which solution would allow the use of SQL to query the stream with the LEAST latency?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "AWS Glue with a custom ETL script to transform the data."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "An Amazon Kinesis Client Library to transform the data and save it to an Amazon ES cluster."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #63"
          },
          "question": {
            "S": "A retail company intends to use machine learning to categorize new products. A labeled dataset of current products was provided to the Data Science team. The dataset includes 1,200 products. The labeled dataset has 15 features for each product such as title dimensions, weight, and price. Each product is labeled as belonging to one of six categories such as books, games, electronics, and movies. Which model should be used for categorizing new products using the provided dataset for training?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "An XGBoost model where the objective parameter is set to multi:softmax"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "A deep convolutional neural network (CNN) with a softmax activation function for the last layer"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "A regression forest where the number of trees is set equal to the number of product categories"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "A DeepAR forecasting model based on a recurrent neural network (RNN)"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #64"
          },
          "question": {
            "S": "A Data Scientist is working on an application that performs sentiment analysis. The validation accuracy is poor, and the Data Scientist thinks that the cause may be a rich vocabulary and a low average frequency of words in the dataset. Which tool should be used to improve the validation accuracy?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Amazon Comprehend syntax analysis and entity detection"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon SageMaker BlazingText cbow mode"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Natural Language Toolkit (NLTK) stemming and stop word removal"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizer"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #65"
          },
          "question": {
            "S": "A Machine Learning Specialist is building a model to predict future employment rates based on a wide range of economic factors. While exploring the data, the Specialist notices that the magnitude of the input features vary greatly. The Specialist does not want variables with a larger magnitude to dominate the model. What should the Specialist do to prepare the data for model training?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Apply quantile binning to group the data into categorical bins to keep any relationships in the data by replacing the magnitude with distribution."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Apply the Cartesian product transformation to create new combinations of fields that are independent of the magnitude."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Apply normalization to ensure each field will have a mean of 0 and a variance of 1 to remove any significant magnitude."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Apply the orthogonal sparse bigram (OSB) transformation to apply a fixed-size sliding window to generate new features of a similar magnitude."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #66"
          },
          "question": {
            "S": "A Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using Amazon Athena. The dataset contains more than 800,000 records stored as plaintext CSV files. Each record contains 200 columns and is approximately 1.5 MB in size. Most queries will span 5 to 10 columns only. How should the Machine Learning Specialist transform the dataset to minimize query runtime?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Convert the records to Apache Parquet format."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Convert the records to JSON format."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Convert the records to GZIP CSV format."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Convert the records to XML format."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #67"
          },
          "question": {
            "S": "A Machine Learning Specialist is developing a daily ETL workflow containing multiple ETL jobs. The workflow consists of the following processes: Start the workflow as soon as data is uploaded to Amazon S3. When all the datasets are available in Amazon S3, start an ETL job to join the uploaded datasets with multiple terabyte-sized datasets already stored in Amazon S3. Store the results of joining datasets in Amazon S3. If one of the jobs fails, send a notification to the Administrator. Which configuration will meet these requirements?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Use AWS Lambda to trigger an AWS Step Functions workflow to wait for dataset uploads to complete in Amazon S3. Use AWS Glue to join the datasets. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Develop the ETL workflow using AWS Lambda to start an Amazon SageMaker notebook instance. Use a lifecycle configuration script to join the datasets and persist the results in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Develop the ETL workflow using AWS Batch to trigger the start of ETL jobs when data is uploaded to Amazon S3. Use AWS Glue to join the datasets in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use AWS Lambda to chain other Lambda functions to read and join the datasets in Amazon S3 as soon as the data is uploaded to Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #68"
          },
          "question": {
            "S": "An agency collects census information within a country to determine healthcare and social program needs by province and city. The census form collects responses for approximately 500 questions from each citizen. Which combination of algorithms would provide the appropriate insights? (Choose two.)"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "The factorization machines (FM) algorithm"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "The Latent Dirichlet Allocation (LDA) algorithm"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "The principal component analysis (PCA) algorithm"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "The k-means algorithm"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "The Random Cut Forest (RCF) algorithm"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #69"
          },
          "question": {
            "S": "A large consumer goods manufacturer has the following products on sale: 34 different toothpaste variants, 48 different toothbrush variants, 43 different mouthwash variants. The entire sales history of all these products is available in Amazon S3. Currently, the company is using custom-built autoregressive integrated moving average (ARIMA) models to forecast demand for these products. The company wants to predict the demand for a new product that will soon be launched. Which solution should a Machine Learning Specialist apply?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Train a custom ARIMA model to forecast demand for the new product."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Train an Amazon SageMaker k-means clustering algorithm to forecast demand for the new product."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Train a custom XGBoost model to forecast demand for the new product."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #70"
          },
          "question": {
            "S": "A Machine Learning Specialist uploads a dataset to an Amazon S3 bucket protected with server-side encryption using AWS KMS. How should the ML Specialist define the Amazon SageMaker notebook instance so it can read the same dataset from Amazon S3?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Define security group(s) to allow all HTTP inbound/outbound traffic and assign those security group(s) to the Amazon SageMaker notebook instance."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Configure the Amazon SageMaker notebook instance to have access to the VPC. Grant permission in the KMS key policy to the notebook's KMS role."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Assign the same KMS key used to encrypt data in Amazon S3 to the Amazon SageMaker notebook instance."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #71"
          },
          "question": {
            "S": "A Data Scientist needs to migrate an existing on-premises ETL process to the cloud. The current process runs at regular time intervals and uses PySpark to combine and format multiple large data sources into a single consolidated output for downstream processing. The Data Scientist has been given the following requirements for the cloud solution: Combine multiple data sources. Reuse existing PySpark logic. Run the solution on the existing schedule. Minimize the number of servers that will need to be managed. Which architecture should the Data Scientist use to build this solution?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Write the raw data to Amazon S3. Schedule an AWS Lambda function to submit a Spark step to a persistent Amazon EMR cluster based on the existing schedule. Use the existing PySpark logic to run the ETL job on the EMR cluster. Output the results to a 'processed' location in Amazon S3 that is accessible for downstream use."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Write the raw data to Amazon S3. Create an AWS Glue ETL job to perform the ETL processing against the input data. Write the ETL job in PySpark to leverage the existing logic. Create a new AWS Glue trigger to trigger the ETL job based on the existing schedule. Configure the output target of the ETL job to write to a 'processed' location in Amazon S3 that is accessible for downstream use."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Write the raw data to Amazon S3. Schedule an AWS Lambda function to run on the existing schedule and process the input data from Amazon S3. Write the Lambda logic in Python and implement the existing PySpark logic to perform the ETL process. Have the Lambda function output the results to a 'processed' location in Amazon S3 that is accessible for downstream use."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use Amazon Kinesis Data Analytics to stream the input data and perform real-time SQL queries against the stream to carry out the required transformations within the stream. Deliver the output results to a 'processed' location in Amazon S3 that is accessible for downstream use."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #72"
          },
          "question": {
            "S": "A Data Scientist is building a model to predict customer churn using a dataset of 100 continuous numerical features. The Marketing team has not provided any insight about which features are relevant for churn prediction. The Marketing team wants to interpret the model and see the direct impact of relevant features on the model outcome. While training a logistic regression model, the Data Scientist observes that there is a wide gap between the training and validation set accuracy. Which methods can the Data Scientist use to improve the model performance and satisfy the Marketing team's needs? (Choose two.)"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Add L1 regularization to the classifier"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Add features to the dataset"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Perform recursive feature elimination"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Perform t-distributed stochastic neighbor embedding (t-SNE)"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Perform linear discriminant analysis"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #73"
          },
          "question": {
            "S": "An aircraft engine manufacturing company is measuring 200 performance metrics in a time-series. Engineers want to detect critical manufacturing defects in near-real time during testing. All of the data needs to be stored for offline analysis. What approach would be the MOST effective to perform near-real time defect detection?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "Use AWS IoT Analytics for ingestion, storage, and further analysis. Use Jupyter notebooks from within AWS IoT Analytics to carry out analysis for anomalies."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use Amazon S3 for ingestion, storage, and further analysis. Use an Amazon EMR cluster to carry out Apache Spark ML k-means clustering to determine anomalies."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use Amazon S3 for ingestion, storage, and further analysis. Use the Amazon SageMaker Random Cut Forest (RCF) algorithm to determine anomalies."
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Use Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut Forest (RCF) to perform anomaly detection. Use Kinesis Data Firehose to store data in Amazon S3 for further analysis."
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #74"
          },
          "question": {
            "S": "A Machine Learning team runs its own training algorithm on Amazon SageMaker. The training algorithm requires external assets. The team needs to submit both its own algorithm code and algorithm-specific parameters to Amazon SageMaker. What combination of services should the team use to build a custom algorithm in Amazon SageMaker? (Choose two.)"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "AWS Secrets Manager"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "AWS CodeStar"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon ECR"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon ECS"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "Amazon S3"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              }
            ]
          }
        }
      }
    },
    {
      "PutRequest": {
        "Item": {
          "QuestionID": {
            "S": "Question #75"
          },
          "question": {
            "S": "A Machine Learning Specialist wants to determine the appropriate SageMakerVariantInvocationsPerInstance setting for an endpoint automatic scaling configuration. The Specialist has performed a load test on a single instance and determined that peak requests per second (RPS) without service degradation is about 20 RPS. As this is the first deployment, the Specialist intends to set the invocation safety factor to 0.5. Based on the stated parameters and given that the invocations per instance setting is measured on a per-minute basis, what should the Specialist set as the SageMakerVariantInvocationsPerInstance setting?"
          },
          "options": {
            "L": [
              {
                "M": {
                  "text": {
                    "S": "10"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "30"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "600"
                  },
                  "correct": {
                    "BOOL": true
                  }
                }
              },
              {
                "M": {
                  "text": {
                    "S": "2,400"
                  },
                  "correct": {
                    "BOOL": false
                  }
                }
              }
            ]
          }
        }
      }
    }
  ]
}